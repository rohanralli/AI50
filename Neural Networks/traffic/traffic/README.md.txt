Originally, I started out with a very basic model of a dense input layer (relu activation), a flattening layer, and a dense output layer (softmax activation). From this, I only elicited an accuracy of about .82. I then decided to utilize convolution and max pooling, and added one layer of both to my program. This got me to an accuracy of about .91. Since this change caused a great increase in accuracy, I thought I'd double down on the convolution/max-pooling. I also went back to the lesson and remembered to add a dropout layer, which I put in right after the flattening layer. This brought accuracy to about a .94, so it helped improve the model at a medium level. Finally, I decided to increase the number of epochs to 12 as well as add another 2 dense layers with an increased number of units before my output layer (with relu activations instead of softmax). This increased my model's accuracy slightly to a final result of .955.